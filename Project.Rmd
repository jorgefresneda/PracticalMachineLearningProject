#Introduction

 
 In this project, we are going to analyse data about weight lifted exercises coming from http://groupware.les.inf.puc-rio.br/har.
 In this experiment, the team asked a number of non-experienced weight lifters to exercise either correctly or incorrectly,

The data is loaded like this:
 
-```{r}
+```{r,message=FALSE}
 # Libraries needed for the whole study
 library(ggplot2)
 library(caret)
 library(dplyr)
 library(randomForest)
-
+library(doMC)
+registerDoMC(cores=4)
+```
+```{r}
 data <- tbl_df(read.csv("pml-training.csv"))
+dim(data)
 ```
 
+We can see the dataset has 160 variables, which is really large.
+
 Cleaning the data
-=================
+-----------------
 
-First, the data contained some division by zero errors. These have been replaced by simple NA using a standard text editor.
+First, the data contained some division by zero errors: these have been replaced by NA using a standard text editor.
 
-In the datasets, many variables
+Then, I removed variables with mostly NA. For this, I create a list of columns I want to keep:
 
+```{r}
+keep_columns <- is.na(data) %>%
+  apply(2, sum) %>%
+  (function(m) { m[m < dim(data)[1]/2] }) %>%
+  names
+```
+
+The last cleaning step consists of removing columns we do not want to train on because they do not make sense for the project.
+The first obvious outliers are `X`, `user_name` and the timestamps. For the timestamps, at most we would want to use a time span
+from the start of the exercise, but never the raw timestamps. A bit less obvious, the `new_window` and `num_window`
+variables indicate when the averaging is done and are probably not very useful in themselved.
 
 ```{r}
-summary(cars)
+to_remove <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
+keep_columns <- keep_columns[which(!keep_columns %in% to_remove)]
+clean_data <- data[keep_columns]
+dim(clean_data)
 ```
 
-You can also embed plots, for example:
+Now, we have only 53 variable lefts, which is more reasonnable, although still very large.
+It is worth nothing that, at that stage, `classe` is the only factor left in the dataset.
+
+Cross-Validation
+----------------
 
-```{r, echo=FALSE}
-plot(cars)
+For the cross validation, we are going to split the samples along the `classe` variable to ensure we have representative of all the classes in the test set. As we have a lot of data and variables, we are going to split the dataset halfway into a training and testing dataset:
+
+```{r}
+inTrain <- createDataPartition(clean_data$classe, p=0.5, list=FALSE)
+training <- clean_data[inTrain,]
+testing <- clean_data[-inTrain,]
+```
+
+However, given the time needed to create a single estimator, we are not going to use this for model selection. It is worth noting that the caret random forest method will itself use bootstrapping to select the best tree.
+
+Preprocessing of the data
+-------------------------
+
+To reduce the number of variables, we use PC analysis, keeping 95% of the variability, and put back the classes:
+
+```{r}
+numTraining <- select(training, -classe)
+preProcPC <- preProcess(numTraining, method=c("pca"), thresh=0.9)
+trainPC <- preProcPC %>%
+  predict(numTraining) %>%
+  mutate(classe=training$classe)
 ```
 
-Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
+Using the Random Forest
+-----------------------
+
+We now use the main classification algorithm: the random forest.
+
+```{r}
+PCForestFit <- train(classe ~ ., data=trainPC, method="rf", prox=TRUE)
+```
+
+Accuracy predictions
+--------------------
+To evaluate the accuracy, we are going to use the testing subset of the original data, and assume all the outcome are equiprobable:
+
+```{r}
+PCtesting <- predict(preProcPC, select(testing, -classe))
+predPCForest <- predict(PCForestFit, PCtesting)
+confusionForest <- confusionMatrix(predPCForest, testing$classe, prevalence=c(A=0.2, B=0.2, C=0.2, D=0.2, E=0.2))
+print(confusionForest)
+```
+
+The interesting line is the Positive Predictive Value, which tells us how likely is the answer is of the predicted class, depending on the prediction. The model then gives us the predicted overall accuracy with a confidence interval:
+
+```{r}
+confusionForest$overall[c("Accuracy", "AccuracyLower", "AccuracyUpper")]
+```
+
+Predictions on the testing dataset
+----------------------------------
+
+```{r}
+test_columns <- keep_columns[which(keep_columns != "classe")]
+test <- tbl_df(read.csv("pml-testing.csv"))[test_columns]
+
+testPC <- predict(preProcPC, test)
+testPredictions <- predict(PCForestFit, testPC)
+print(testPredictions)
+```
